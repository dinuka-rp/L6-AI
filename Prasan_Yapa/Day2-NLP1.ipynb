{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mr.Prasan Yapa - NLP1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdzdjLCg3S3AJQgfVCZBQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinuka-rp/L6-AI/blob/main/Prasan_Yapa/Day2-NLP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyU0ro_ih-hp"
      },
      "source": [
        "# Tokenization, Stemming, and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ7yh9aHYtbK",
        "outputId": "da77ed70-ecd1-4cf6-98d8-5672ebc5e99b"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfW6BeziQDh"
      },
      "source": [
        "We\n",
        "will be using the English language model. The language model is used to perform a variety of\n",
        "NLP tasks,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gzsc7-tiVYn",
        "outputId": "2aa6dea7-c5a4-4aed-d9e2-1fd7d2334dc3"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 27.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFrow9GPii0p"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlTDJYR5ifKI"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjQ3uMCEiovz"
      },
      "source": [
        "## load the spaCy language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoW8WW80ieNH"
      },
      "source": [
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgHWIvqjjKg7"
      },
      "source": [
        "## Create a small document using this model.\n",
        "A document can be a sentence or a group of sentences and can have unlimited length. The following script creates a simple spaCy document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHl7QwNXiyo4"
      },
      "source": [
        "sentence = sp('Manchester United is looking to sign a forward for $90 million')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXaPHK4i6Ut"
      },
      "source": [
        "A token simply refers to an individual part of a sentence having some semantic value. Let's see\n",
        "what tokens we have in our document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCscBvyBi620",
        "outputId": "693c46cb-970d-42aa-f1ba-cd51099cab72"
      },
      "source": [
        "for word in sentence:\n",
        "  print(word.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manchester\n",
            "United\n",
            "is\n",
            "looking\n",
            "to\n",
            "sign\n",
            "a\n",
            "forward\n",
            "for\n",
            "$\n",
            "90\n",
            "million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5nKbaN6jfPw"
      },
      "source": [
        "We can also see the parts of speech\n",
        "of each of these tokens using the .pos_ attribute shown below.\n",
        "\n",
        "*\"Manchester\" has been tagged as a proper noun,*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZn3ishGjgKE",
        "outputId": "1bf32798-ba83-43b6-d337-63686ab0d21f"
      },
      "source": [
        "for word in sentence:\n",
        "  print(word.text, word.pos_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manchester PROPN\n",
            "United PROPN\n",
            "is AUX\n",
            "looking VERB\n",
            "to PART\n",
            "sign VERB\n",
            "a DET\n",
            "forward NOUN\n",
            "for ADP\n",
            "$ SYM\n",
            "90 NUM\n",
            "million NUM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc8uJUo2kMzw"
      },
      "source": [
        "In addition to printing the words, you can also print sentences from a document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4uAlX4DkPtP",
        "outputId": "b331c2d3-05f9-4989-ecf3-07cd4200e2e4"
      },
      "source": [
        "document = sp('Hello all, welcome to Natural Language Processing class. What is the content for today?')\n",
        "for sentence in document.sents:\n",
        "  print(sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello all, welcome to Natural Language Processing class.\n",
            "What is the content for today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5vI0LJkngx"
      },
      "source": [
        "## spaCy tokenization in detail.\n",
        "\n",
        "Create a new document using the following script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uALM77bmkqVA",
        "outputId": "16eee960-69a0-46b0-b002-a98d930598ed"
      },
      "source": [
        "sentence1 = sp('\"They\\'re leaving U.K. for U.S.A.\"')\n",
        "print(sentence1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"They're leaving U.K. for U.S.A.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UpdDT2Ok2Ie"
      },
      "source": [
        "This sentence contains quotes at the beginning and at the end. It also contains\n",
        "punctuation marks in abbreviations \"U.K\" and \"U.S.A.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it2ocptck87c",
        "outputId": "b77c8633-cec0-47fa-a2fb-7770e3dcdceb"
      },
      "source": [
        "for word in sentence1:\n",
        "  print(word.text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "They\n",
            "'re\n",
            "leaving\n",
            "U.K.\n",
            "for\n",
            "U.S.A.\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqZlR8bdlSpf"
      },
      "source": [
        "another tokenization example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Bo65QtlRzI",
        "outputId": "4f42e6a7-1583-4f23-809a-abab1dabe271"
      },
      "source": [
        "sentence2 = sp(\"Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\")\n",
        "print(sentence2)\n",
        "\n",
        "for word in sentence2:\n",
        "  print(word.text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\n",
            "Hello\n",
            ",\n",
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "email\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abc-xyz@gmai.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcobBnnRlfmN"
      },
      "source": [
        "the output that spaCy was actually able to detect the email and it did not\n",
        "tokenize it despite having a \"-\". On the other hand, the word \"non-vegetarian\" was tokenized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WITw4loWl3E2"
      },
      "source": [
        "In addition to tokenizing the documents to words, you can also find if the word is an entity\n",
        "such as a company, place, building, currency, institution, etc. To get the named entities from a\n",
        "document, you have to use the ents attribute. Let's retrieve the named entities from the above\n",
        "sentence. Execute the following script.\n",
        "\n",
        "we print the text of the entity, the label of the entity and the detail of the\n",
        "entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZLXlSoJl47R",
        "outputId": "5c330718-93fc-4580-9a93-14fea20a5a81"
      },
      "source": [
        "sentence3 = sp('Manchester United is looking to sign Harry Kane for $90 million')\n",
        "for entity in sentence3.ents:\n",
        "  print(entity.text + ' - ' + entity.label_ + ' - ' +\n",
        "str(spacy.explain(entity.label_)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manchester United - PERSON - People, including fictional\n",
            "Harry Kane - PERSON - People, including fictional\n",
            "$90 million - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvDEkM6EnO6D"
      },
      "source": [
        "Nouns can also be detected.\n",
        "\n",
        "A noun can be a named entity as well and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDA_G04OnHoh",
        "outputId": "f461ac87-716e-44d6-fb78-b9c4d91fe2f6"
      },
      "source": [
        "sentence4 = sp('Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million')\n",
        "for noun in sentence4.noun_chunks:\n",
        "  print(noun.text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manchester United\n",
            "Harry Kane\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvt6GHLcmfvd"
      },
      "source": [
        "## Stemming using NTLK\n",
        "\n",
        "Stemming refers to reducing a word to its root form. While performing natural language\n",
        "processing tasks, you will encounter various scenarios where you find different words with the\n",
        "same root. For instance, compute, computer, computing, computed, etc. You may want to\n",
        "reduce the words to their root form for the sake of uniformity. This is where stemming comes\n",
        "in to play.\n",
        "It might be surprising to you but spaCy doesn't contain any function for stemming as it relies\n",
        "on lemmatization only. Therefore, in this section, we will use NLTK for stemming.\n",
        "\n",
        "There are two types of stemmers in NLTK: Porter Stemmer and Snowball stemmers. Both of\n",
        "them have been implemented using different algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12luy-MDrqEG"
      },
      "source": [
        "### Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozCT6joercT-",
        "outputId": "79332501-4995-4296-f404-cba186014059"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "\n",
        "for token in tokens:\n",
        "  print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8_SL1_rjzb"
      },
      "source": [
        "You can see that all the four words have been reduced to \"comput\" which actually isn't a word\n",
        "at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIP9_fPsrl6e"
      },
      "source": [
        "Snowball Stemmer is a slightly improved version of the Porter Stemmer and is usually preferred\n",
        "over the latter. Let's see Snowball Stemmer in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keI1XTxOrw88"
      },
      "source": [
        "### Snowball Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jMw3-4RrkmS",
        "outputId": "1e2d32f8-a6fb-4a1a-e41c-b81ea82e713a"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "\n",
        "for token in tokens:\n",
        "  print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjKeWyYhr7OW"
      },
      "source": [
        "You can see that the results are the same. We still got \"comput\" as the stem. Again, this word\n",
        "\"comput\" actually isn't a dictionary word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xwkUtW4r-oi"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization converts words in the second or third forms to their first form variants.\n",
        "\n",
        "Though we could not perform stemming with spaCy, we can perform lemmatization using\n",
        "spaCy. To do so, we need to use the `lemma_attribute` on the spaCy document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q3SrAY3sLo5",
        "outputId": "2f60b877-c562-4c0e-ecd9-1585984383d4"
      },
      "source": [
        "sentence5 = sp('compute computer computed computing')\n",
        "\n",
        "for word in sentence5:\n",
        "  print(word.text, word.lemma_)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute compute\n",
            "computer computer\n",
            "computed compute\n",
            "computing computing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9RHebeHsey3"
      },
      "source": [
        "You can see that unlike stemming where the root we got was \"comput\", the roots that we got\n",
        "here are actual words in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmEOWAEfslwt",
        "outputId": "07dd69b2-79bb-42ca-f22c-3f9f4b426b6d"
      },
      "source": [
        "sentence6 = sp('A letter has been written, asking him to be released')\n",
        "\n",
        "for word in sentence6:\n",
        "  print(word.text + ' ===>', word.lemma_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A ===> a\n",
            "letter ===> letter\n",
            "has ===> have\n",
            "been ===> be\n",
            "written ===> write\n",
            ", ===> ,\n",
            "asking ===> ask\n",
            "him ===> -PRON-\n",
            "to ===> to\n",
            "be ===> be\n",
            "released ===> release\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0GV3Fdzsv2q"
      },
      "source": [
        "You can clearly see from the output that the words in second and third forms, such as \"written\",\n",
        "\"released\", etc. have been converted to the first form i.e. \"write\" and \"release\"."
      ]
    }
  ]
}