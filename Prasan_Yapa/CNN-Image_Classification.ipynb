{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mr.Prasan Yapa - Image ClassificationCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNNKCF9PcWG0GlTsRKNEObd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinuka-rp/L6-AI/blob/main/Prasan_Yapa/CNN-Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJfLLEZNddUV"
      },
      "source": [
        "# Image Recognition with CNN using TensorFlow and Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eK9ncw-jSM8"
      },
      "source": [
        " TensorFlow is an open source library created for Python by the Google Brain team. TensorFlow compiles many different algorithms and models together, enabling the user to implement deep neural networks for use in tasks like image recognition/classification and natural language processing.\n",
        "\n",
        "Keras is a high-level API that can use TensorFlow's\n",
        "functions underneath."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc1ab2Qnjv9n"
      },
      "source": [
        "Image recognition refers to the task of inputting an image into a neural network and having it output label for that image. The label that the network outputs will correspond to a pre-defined class.\n",
        "\n",
        "Features are the elements of the data that you care about which will be fed through the network. In the specific case of image recognition, the features are the groups of pixels, like edges and points, of an object that the network will analyze for patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCSv_UsokEho"
      },
      "source": [
        "CIFAR-10 dataset. CIFAR-10 is a large image dataset containing over 60,000 images representing 10 different classes of objects like cats, planes, and cars. \n",
        "\n",
        "One great thing about the CIFAR-10 dataset is that it comes prepackaged with Keras, so it is very easy to load up the dataset and the images need very little preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzCbcyhwwmIm"
      },
      "source": [
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRfT4scadJVi"
      },
      "source": [
        "import numpy\n",
        "from keras.datasets import cifar10\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TphMHNTpkv62"
      },
      "source": [
        "We're going to be using a random seed here so that the results achieved in this lab can be replicated by you, which is why we need numpy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxACqnFrwslf"
      },
      "source": [
        "seed = 21\n",
        "numpy.random.seed(seed)\n",
        "# https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKwU12Jpk8Eg"
      },
      "source": [
        "Now let's load in the dataset. We can do so simply by specifying which variables we want to load the data into, and then using the load_data() function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5-FrUpnxmTT"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym-LgBftk92M"
      },
      "source": [
        "In most cases you will need to do some preprocessing of your data to get it ready for use, but since we are using a prepackaged dataset, very little preprocessing needs to be done. One thing we want to do is normalize the input data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2b5Hj_blV40"
      },
      "source": [
        "So, in order to normalize the data, we can simply divide the image values by 255. To do this we first need to make the data a float type, since they are currently integers. We can do this by using the `astype()` Numpy command and then declaring what data type we want.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q9vcppIla87"
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWroCWtSltI9"
      },
      "source": [
        "We are effectively doing binary classification here because an image either belongs to one class or it doesn't, it can't fall somewhere in-between. The Numpy command `to_categorical()` is used to one-hot encode. This is why we imported the `np_utils` function from Keras, as it contains `to_categorical().`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4uK9w2_lzbo",
        "outputId": "e1dac212-49be-4e9b-fc20-ac00436d94dd"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# print(y_test)\n",
        "print(num_classes)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En5b13jpmm6X"
      },
      "source": [
        "## Design the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08N0JGCEmpeW"
      },
      "source": [
        "The first thing to do is define the format we would like to use for the model, Keras has several different formats or blueprints to build models on, but Sequential is the most commonly used, and for that reason, we have imported it from Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-DW517nmo4T"
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38Kx2kpoxtp"
      },
      "source": [
        "When implementing these in Keras, we must specify the number of channels/filters we want (that's the 32 below), the size of the filter we want (3 x 3 in this case), the input shape (when creating the first layer) and the activation and padding we need. As mentioned, relu is the most common activation, and padding='same' just means we aren't changing the size of the image at all.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk1wPsQWo_OU"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
        "model.add(Activation('relu'))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C63q6-_pEhq"
      },
      "source": [
        "We will make a dropout layer to prevent overfitting, which functions by *randomly eliminating some of the connections between the layers*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txVXPQFLpFa8"
      },
      "source": [
        "model.add(Dropout(0.2))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLMJ7YjKpKwl"
      },
      "source": [
        "We may also want to do batch normalization here. Batch Normalization normalizes the inputs heading into the next layer, ensuring that the network always creates activations with the same distribution that we desire.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKhqK4UVpNy4"
      },
      "source": [
        "model.add(BatchNormalization())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWQRK6kupR-x"
      },
      "source": [
        "Now comes another convolutional layer, but the filter size increases so the network can learn more complex representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGBo1DyopTBA"
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f3YrxLDpcEL"
      },
      "source": [
        "Here's the pooling layer, as discussed before this helps make the image classifier more robust so it can learn relevant patterns. There's also the dropout and batch normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XwbJNU9pewG"
      },
      "source": [
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6UsQ_thplvs"
      },
      "source": [
        "That's the basic flow for the first half of a CNN implementation:\n",
        "\n",
        "\n",
        "1.   Convolutional\n",
        "2.   activation\n",
        "3.   dropout\n",
        "4.   pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWTrr9OSqJO2"
      },
      "source": [
        "It's important not to have too many pooling layers, as *each pooling discards some data*.\n",
        "Pooling too often will lead to there being almost nothing for the densely connected layers to learn about when the data reaches them. \n",
        "\n",
        "You can now repeat these layers to give your network more representations to work off.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpa1eBfeqUKI"
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prDM6fZ9qsYK"
      },
      "source": [
        "After we are done with the convolutional layers, we need to Flatten the data, which is why we imported the function above.\n",
        "We'll also add a layer of dropout again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF8zrdGeqwI9"
      },
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVE1bjeCq2PC"
      },
      "source": [
        "Now we make use of the Dense import and create the first densely connected layer. We need to specify the number of neurons in the dense layer.\n",
        "\n",
        "Note that the numbers of neurons in succeeding layers decreases, *eventually approaching the same number of neurons as there are classes in the dataset*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VnjQoVSrCNu"
      },
      "source": [
        "from keras.constraints import maxnorm\n",
        "\n",
        "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn49Sct2rEHn"
      },
      "source": [
        "Finally, the ***softmax activation function*** *selects the neuron with the highest probability as its output, voting that the image belongs to that class*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXWD1VaTrOX4"
      },
      "source": [
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oNwZJmzryZL"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ08jaUHr1C0"
      },
      "source": [
        "Now that we've designed the model we want to use, we just have to compile it. Let's specify the number of epochs we want to train for, as well as the optimizer we want to use.\n",
        "\n",
        "The optimizer is what will tune the weights in your network to approach the point of lowest loss.\n",
        "The *Adam* algorithm is one of the most commonly used optimizers because it gives great performance on most problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsCivVGOseo0"
      },
      "source": [
        "epochs = 25\n",
        "optimizer = 'Adam'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUYVzcUjtcqZ"
      },
      "source": [
        "Let's now compile the model with our chosen parameters. Let's also specify a metric to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN9M0Xh-tdOk",
        "outputId": "f9a66e2f-7383-4bcb-c9d4-5e5eefb64e0a"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,264,458\n",
            "Trainable params: 2,263,114\n",
            "Non-trainable params: 1,344\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_4uisllt23s"
      },
      "source": [
        "Now we get to training the model. To do this, all we have to do is call the `fit()` function on the model and pass in the chosen parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUqcsqjmt9r0",
        "outputId": "6806682d-7ab4-4ba1-f419-8e9579e2951b"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs,\n",
        "batch_size=64)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "782/782 [==============================] - 402s 512ms/step - loss: 1.5098 - accuracy: 0.4655 - val_loss: 1.1239 - val_accuracy: 0.5873\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 400s 512ms/step - loss: 1.0231 - accuracy: 0.6390 - val_loss: 0.8155 - val_accuracy: 0.7144\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 402s 514ms/step - loss: 0.8524 - accuracy: 0.6993 - val_loss: 0.7520 - val_accuracy: 0.7378\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 404s 517ms/step - loss: 0.7625 - accuracy: 0.7335 - val_loss: 0.6692 - val_accuracy: 0.7692\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 404s 517ms/step - loss: 0.7100 - accuracy: 0.7501 - val_loss: 0.6568 - val_accuracy: 0.7696\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 403s 516ms/step - loss: 0.6596 - accuracy: 0.7685 - val_loss: 0.6555 - val_accuracy: 0.7729\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 403s 515ms/step - loss: 0.6268 - accuracy: 0.7803 - val_loss: 0.6580 - val_accuracy: 0.7707\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 402s 514ms/step - loss: 0.6056 - accuracy: 0.7887 - val_loss: 0.6122 - val_accuracy: 0.7918\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 402s 514ms/step - loss: 0.5842 - accuracy: 0.7961 - val_loss: 0.6353 - val_accuracy: 0.7781\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 403s 515ms/step - loss: 0.5574 - accuracy: 0.8069 - val_loss: 0.5999 - val_accuracy: 0.7930\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 402s 514ms/step - loss: 0.5379 - accuracy: 0.8143 - val_loss: 0.5795 - val_accuracy: 0.8033\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 401s 513ms/step - loss: 0.5249 - accuracy: 0.8180 - val_loss: 0.6532 - val_accuracy: 0.7765\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 401s 513ms/step - loss: 0.5130 - accuracy: 0.8201 - val_loss: 0.5623 - val_accuracy: 0.8013\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 400s 512ms/step - loss: 0.4951 - accuracy: 0.8278 - val_loss: 0.5584 - val_accuracy: 0.8116\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 401s 513ms/step - loss: 0.4886 - accuracy: 0.8293 - val_loss: 0.5507 - val_accuracy: 0.8090\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 404s 517ms/step - loss: 0.4771 - accuracy: 0.8315 - val_loss: 0.6054 - val_accuracy: 0.7979\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 405s 518ms/step - loss: 0.4692 - accuracy: 0.8375 - val_loss: 0.5687 - val_accuracy: 0.8063\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 399s 511ms/step - loss: 0.4617 - accuracy: 0.8380 - val_loss: 0.5666 - val_accuracy: 0.8093\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 399s 511ms/step - loss: 0.4536 - accuracy: 0.8423 - val_loss: 0.5660 - val_accuracy: 0.8062\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 400s 512ms/step - loss: 0.4448 - accuracy: 0.8443 - val_loss: 0.5107 - val_accuracy: 0.8272\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 404s 517ms/step - loss: 0.4411 - accuracy: 0.8454 - val_loss: 0.5487 - val_accuracy: 0.8160\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 405s 519ms/step - loss: 0.4309 - accuracy: 0.8480 - val_loss: 0.5430 - val_accuracy: 0.8217\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 406s 520ms/step - loss: 0.4321 - accuracy: 0.8479 - val_loss: 0.5043 - val_accuracy: 0.8340\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 407s 521ms/step - loss: 0.4214 - accuracy: 0.8532 - val_loss: 0.5749 - val_accuracy: 0.8066\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 403s 516ms/step - loss: 0.4146 - accuracy: 0.8547 - val_loss: 0.5411 - val_accuracy: 0.8176\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd33cc8f190>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifQKoyTiuBZk"
      },
      "source": [
        "Finally, we can evaluate the model and see how it performed. Just call `model.evaluate()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dV2HsKquDd_",
        "outputId": "d6bba9a9-e611-4bba-d104-b83b4fb4e444"
      },
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 81.76%\n"
          ]
        }
      ]
    }
  ]
}